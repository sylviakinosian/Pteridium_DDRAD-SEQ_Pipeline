---
title: "*Pteridium* GBS Pipeline"
author: "Sylvia Kinosian"
output: html_document
---

This pipeline was designed by Zach Gompert and edited by Sylvia Kinosian. All of the associated files can be found at: https://github.com/sylviakinosian/Pteridium_GBS_Pipeline 

The following protocol was designed to process .FASTQ files from ddRADseq data for 4 species of the fern genus *Pteridium*: *P. aquilinum* (diploid), *P. esculentum* (diploid), *P. semihaustatum* (tetraploid), and *P. caudatum* (tetraploid). There is no reference genome for *Pteridium*, so we create a reference *de novo* using the diploid species only. When calling variants, we chose to use the GATK HaplotypeCaller because it allows the user to specify ploidy. We clustered the diploids and tetraploids separately (each run specifying ), then recombined the resulting SNPs (additive, not averaged).

Further analyses are done using the program ENTROPY to estimate admixture among the individuals. The goal of this analysis is to determine the parentage of the tetraploid species, which are hypothesized to be allotetraploids. We are also interested in the admixture within the diploid species, with regards to revising taxonomic rank.

There are three main steps to the GBS Pipeline (parsing, building the reference, and calling variants); running ENTROPY also has roughly three steps (seeding values with DAPC, running ENTROPY, visualizing and further analysis in R).

# {.tabset .tabset-fade}

## GBS Pipeline

### **1) Parse barcodes and split .FASTQ by individual**
This first step uses perl scripts to parse the barcodes from the raw GBS .FASTQ data and then split that raw file into indiviual .FASTQ files. Note that all Perl were run using Perl 5 (http://www.perl.org).

### a. 
The first scipt, parse\_barcodes768.pl, requires two files: a barcodes file and the raw GBS.FASTQ file. The barcodes.txt file has three columns: the index name, barcode, and sample name for each indiviual. See barcodes.txt for an example. 

Usage:
```{bash eval=FALSE}
parse_barcodes768.pl barcodes.txt Pter_01_S17_L002_R1_001.fastq
```

### b. 
The next script, splitFastq\_ms.pl, requires two files: a list of individuals and the parsed.GBS.FASTQ file (one of the output file from parse\_barcodes768.pl). The individuals.txt file is simply a list of the names for each individual you would like to create a .FASTQ file for (a text file contain one column, with one name per row, no header).

Usage:
```{bash eval=FALSE}
splitFastq_ms.pl individuals.txt parsed_Pter_01_S17_L002_R1_001.fastq
```
Now, you should have a .FASTQ file for each individual listed in the indivduals.txt file. 

###  **2) Building our reference**

#### PLEASE NOTE: this is for the diploid species ONLY (*P. aquilinum* and *P. esculentum*). 

Now that we have extracted the raw information for each individual, it's time to build our reference genome. There is no reference genome available for *Pteridium*, so we are going to build one *de novo* using the two diploid species. Since the tetraploids are most likely allotetraploids (of hybrid origin) we are only using the diploids because that will capture most of the sequence variation and is cleaner than dealing with the possibly divergent sequences on the tetraploid hybrids.

We decided to build the reference genome by clustering similar sequences within species, and then combining those sequences across individuals. For the final reference, we made sure that each contig was represented in BOTH diploid species.

**Before starting**, transfer the .FASTQ files for the diploid individuals to separate folders. This will make the clustering within each species much easier.

Below is a command to copy a list of files into different folder. The file\_list.txt is a list of the file names (name.fastq) for the individuals you would like to move.

```{bash eval=FALSE}
xargs cp -t /path/to/folder < file_list.txt
```

### Step 1: Cluster highly similar sequences in individual .FASTQ files

#### a.
The first part of this step is to convert the .FASTQ files into .FASTA files using the program SEQTK (v. 1.2-r102-dirty). See the script seqtk.sh (below) for an example of how to loop through the files in a directory.

```{bash eval=FALSE}
for i in *.fastq; do
    id=$(echo $i | cut -f1 -d.)
    echo $id
	seqtk seq -a $i > $id.fasta
	done
```

SEQTK parameters - convert FASTQ to FASTA:

seqtk seq -a file.tq > file.fa

This purely converts file formats, no changes. See https://github.com/lh3/seqtk for additional options for converting to FASTA format.

#### b.
The second part is to use the program VSEARCH (v. 2.4.2) to cluster sequences **within each individual** with a 98% similarity to create "centroids". 

usage of vsearch.sh:

```{bash eval=FALSE}
vsearch.sh 0.98 centroids
```

VSEARCH command "under the hood" of vsearch.sh:
```{bash eval=FALSE}
vsearch --cluster_fast indivdual.fasta --id 0.98 --iddef 2 --threads 8 --centroids centroids/centroids98_individual.fasta
```

Parameters - see VSEARCH documentation (https://github.com/torognes/vsearch, Getting Help section):

--cluster\_fast --> clusterize the fasta sequencing in the given file; automatically sorted by decreasing sequence abundance<br> 
--id 0.98 --> cluster sequences at 98% similarity; pairwise identify defined as the number of (matching columns)/(alignment length - terminal gaps); can be modified by --iddef <br>
--iddef 2 --> change the pairwise identity definition used in --id. This is the default value but we chose to include it in case the default changes.
--threads 8 --> number of computations threads to use<br>
--centroids --> file name or folder to place clustered sequences in; the centroid is the sequence that seeded the cluster (ie. the first sequence of the cluster)

### Step 2: combine centroids from preceeding runs and cluster at 92% similarity

--consout *filename* --> Output ONLY cluster consensus sequences to *filename*. For each cluster, a multiple alignment is computed, and a consensus sequence is constructed by taking the majority symbol (nucleotide or gap) from each column of the alignment.<br>
--msaout *filename* --> Output BOTH a multiple sequence alignment AND a consensus sequence for each cluster to *filename*.
--cluster\_fast --> clusterize the fasta sequencing in the given file; automatically sorted by decreasing sequence abundance<br> 
--id 0.92 --> cluster sequences at 92% similarity<br>
--iddef 2 --> change the pairwise identity definition used in --id. This is the default value but we chose to include it in case the default changes.
--threads 10 --> number of computations threads to use<br>

#### a. *P. aquilinum*
```{bash eval=FALSE}
cat *.fasta > aqui_consensus.fasta

vsearch --cluster_fast aqui_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92aquiCons.fasta --msaout 92aquiMsa.fasta
```

#### b. *P. esculentum*
```{bash eval=FALSE}
cat *.fasta escu_consensus.fasta

vsearch --cluster_fast consensus_escu.fasta --threads 10 --iddef 2 --id 0.92 --consout cons_escu92.fasta --msaout msa_escu92.fasta
```

### Step 3: Cluster at 84% similarity, using the file clustered at 92% similarity as the "reference". Then remove paralogs

#### a. *P. aquilinum* - cluster at 84% similarity, using the file clustered at 92% similarity as the "reference"

```{bash eval=FALSE}
vsearch --cluster_fast 92aquiCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84aquiCons.fasta --msaout 84aquiMsa.fasta
```

#### b. *P. esculentum* - cluster at 84% similarity, using the file clustered at 92% similarity as the "reference"

```{bash eval=FALSE}
vsearch --cluster_fast 92escuCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84escuCons.fasta --msaout 84escuMsa.fasta
```

#### c. Remove collapsed clusters (paralogs) from files clustered at 84% similarity

The remove\_collapsed\_clusters.py script removes all entries that have (the 2nd) seqs > 1.

```{bash eval=FALSE}
./remove_collapsed_clusters.py 84aquiCons.fasta RCCaqui.fasta
sta

902952 uncollapsed clusters found
```

```{bash eval=FALSE}
./remove_collapsed_clusters.py 84escuCons.fasta RCCescu.fasta

444932 uncollapsed clusters found
```

The resulting RCC\*.fasta files will be used in step 5

### Step 4: combine aqui and escu, re-run vsearch clustering and filter

#### a. BEFORE COMBINING
Make sure your *aquilinum* and *esculentum* inds are marked separatley within the RCC\*.fasta files. This will make checking to see if each final contig is represented by each species much easier.

```{bash eval=FALSE}
sed 's/^>centroid=centroid=/>centroid=centroid=a/g' RCCaqui.fasta > aRCCaqui.fasta

sed 's/^>centroid=centroid=/>centroid=centroid=e/g' RCCescu.fasta > eRCCescu.fasta
```

####b. Combine *aquilinum* and *esculentum*

```{bash eval=FALSE}
cat aRCCaqui.fasta eRCCescu.fasta > ae_cons.fasta
```

####c. Re-run vsearch with an id (% similarity) of your choice (92,88,86,84...)

This clustering step in done to ensure that the individual contigs isolated are present in both *aquilinum* and *esculentum*.

```{bash eval=FALSE}
# cluster at 88% similarity
vsearch --cluster_fast ae_cons.fasta --threads 10 --iddef 2 --id 0.88 --consout 88ae_consout.fasta --msaout 88ae_msaout.fasta

# remove all clusters that are present in only one species
./presence_filter.pl 88ae_msaout.fasta

finished, retained 85422 contigs
```

```{bash eval=FALSE}
# cluster at 84% similarity
vsearch --cluster_fast ae_cons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84ae_consout.fasta --msaout 84ae_msaout.fasta

# remove all clusters that are present in only one species
./presence_filter.pl 84ae_msaout.fasta

finished, retained 105428 contigs
```

We chose to use the 84% similarity. The proided us with a large number of contigs present in both species. We are going to apply some much stricter filtering parameters later on in the variant calling step, so it is better to start off with a few more contigs / material in general.

Hooray! Now you have a fresh *de novo* reference with which to align your parsed .FASTQ files!

## Alignment of parsed reads

### Step 1: Prepare the reference sequence

#### a. Index the reference (consensus) sequence. 

Here, we used the Burrow-Wheeler Aligner (BWA v. 0.7.10) to index our reference genome. This give the squence position points for the alignment later on.

```{bash eval=FALSE}
bwa index ae_consensus_final.fasta 
```

#### b. Picard tools to create a dictionary

We used Java (OpenJDK) v. 1.8.0 and PicardTools v. 2.9.0

```{bash eval=FALSE}
java -jar picard.jar CreateSequenceDictionary REFERENCE=ae_consensus_final.fasta OUTPUT=ae_consensus_final.dict
```
#### c. Creating the fasta index file

We used SAMTOOLS v. 1.5

```{bash eval=FALSE}
samtools faidx ae_consensus_final.fasta
```

### Step 2: Align parsed reads (from ALL individual .FASTQ files) to the *de novo* reference

#### a. Align individuals with BWA ALN

See script `bwa_aln.sh`

```{bash eval=FALSE}
#! /bin/bash

REF='/uufs/chpc.utah.edu/common/home/wolf-group2/skinosian/3pteridium/parse/fastq/ae_consensus_final.fasta'

for i in *.fastq;
do
ids=$(echo $i | cut -f1 -d.)
echo $ids

/uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa aln -n 4 -l 20 -k 2 -t 8 -q 10 -f $ids.sai $REF $i

/uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa samse -n 1 -r "@RG\tID:$ids\tLB:$ids\tSM:$ids\tPL:ILLUMINA" -f $ids.sam $REF $ids.sai $i

done
```

The output is .SAM files for each individual

#### b. Convert files from .SAM to .BAM, sort, and index the individuals using SAMTOOLS

```{bash eval=FALSE}
samtools view -o *.bam *.sam

samtools sort -o *.sorted.bam *.bam
 
samtools index -b *.sorted.bam
```

Because there were about 100 individuals, we used a fork manager to run this through the University of Utah Center for High Performance Computing cluster (CHPC). The result is a .bam, .sorted.bam, and .sorted.bam.bai file for each individual. See the script fork\_view\_sort\_index.pl for an example.

## Variants Calling

To call variants, we used the GATK HaplotypeCaller (v. 3.8.0) because of its ability to specify ploidy. We called variants separately for the diploids and tetraploids.

#### Diploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R ae_consensus_final -I diploid_bams.list --genotyping_mode DISCOVERY -ploidy 2 -o 2ae_rawVar.vcf -out_mode EMIT_VARIANTS_ONLY
```

#### Tetraploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R ae_consensus_final -I tetraploid_bams.list --genotyping_mode DISCOVERY -ploidy 4 -o 4ae_rawVar.g.vcf -out_mode EMIT_VARIANTS_ONLY --variant_index_type LINEAR --variant_index_parameter 128000
```

```{bash eval=FALSE}
grep -v ^# ae.vcf | cut -f 8 | perl -p -i -e 's/DP=(\d+);\S+/\1/' > depth.txt
```

### Step 1: filter VCFs

VCFTOOLS (v. 0.1.15) can be used to filter diploids (see below), but because it does not support tetraploids we created a custom Python script to filter instead.

```{bash eval=FALSE}
vcftools --remove-filtered-all --remove-indels --maf 0.1 --max-maf 0.99 --min-meanDP 2.0 --max-missing 0.3 --minQ 20 --recode-INFO-all --recode --vcf 2ae\_rawVar.vcf
```

The script vcfFilter.py filters based on read depth (minCoverage), alternative alleles(minAltRds), fixed loci (notFixed), and mapping quality (mapQual). These variables can be altered within the file to achieve the desired filtering affect (see **stringency variable** in script below). 

BOTH VCF files need to be filtered this way (2ae\_rawVar.vcf and 4ae\_rawVar.vcf).

### Step 2: Find the intersection of variants in diploids and tetraploids

Since we now have two VCF files, we need to combine them again somehow. To do this, we find the **intersection** of the variants in both files, subset, and then re-combine.

#### a. Intersection of variants

```{bash eval=FALSE}
perl vcf_checker.pl filtered_ae2.vcf filtered_4ae.vcf
```

Output is a list of the contigs present in both VCF files called matches.txt

#### b. Subset VCF files with list of intersection matches

Do this for both files.

```{bash eval=FALSE}
perl subsetVcf.pl matches.vcf filtered_ae2.vcf

perl subsetVcf.pl matches.vcf filtered_ae4.vcf
```

Output is two files: sub\_filtered\_ae2.vcf and sub\_filtered\_ae4.vcf

#### c. Combine files

What we are doing here is appending the data from one file onto the end of each matching contig in the other file.

```{bash eval=FALSE}
perl combine.pl sub\_filtered\_ae2.vcf sub\_filtered\_4ae.vcf
```

This outputs a file called aeAll.vcf

Our original sample size was 101 plus 15 replicates. Three of the samples were an outgroup, which we decided not to use in the admixture analysis. A fourth sample did not qork in sequencing; its initial FASTQ file was empty. After calling variants, we checked our replicates against their associated indiviual, and determined that all replicates had amplified properly and made it through the pipeline to match their original ID. We then combinded replicates to their associated individual and re-did the variant calling.


## Estimation of Admixture

### **Entropy**

Before getting started with Entropy, we need to convert our VCF file to a GL (Genotype Likelihood) file.

We used the perl script vcf2gl.pl to convert our filtered vcf to the simpler .gl format for downstream analysis.

For diploids:

```{bash eval=FALSE}
perl vcf2gl.pl aeAll.vcf
```
This outputs a file called out.recode.gl

Next we are going to convert the GL file to a matrix that we can use in R with DAPC.

```{bash eval=FALSE}
perl gl2genest.pl out.recode.gl
```
This outputs a file called pntest\_out.recode.gl

### Discriminant Analysis of Principle Components 

Among the diploid species, *Pteridium aquilinum* and *P. esculentum*, and the tetraploid species, *P. semihaustatum* and *P. caudatum*, there are 15 sub-species. These distinctions are based mostly on morphology, and so testing the population structure among them will help distinguish the validity of these biological species and sub-species.

##### a. seed entropy with values from DAPC

Using the R package ADEGENET (v. 2.1.1), run a Discriminate Analysis of Principle Componets (DAPC function) to seed values in ENTROPY so we don't get label swapping. We followed the [DAPC vignette](adegenet.r-forge.r-project.org/files/tutorial-dapc.pdf).

```{r eval=FALSE}
library(adegenet)

# read in genotype matrix
d <- read.table("pntest_out.recode.vcf", header = F)

# transform data
dt <- t(d)

# convert to genind object
dg <- df2genind(dt, sep = " ", ploidy = 2)

grp <- find.clusters(dg, max.n.clust = 15)
# number of PCs retained: 60
# number of clusters: 2

head(grp$grp, 97)

# get likelihood assignments

dapc1 <- dapc(dg, grp$grp)
# PCs 60
# discriminant dunctions: 1

write.table(dapc1$posterior, "k_est.txt")
```

##### b. run ENTROPY

```{bash eval=FALSE}
./entropy -b 2000 -t 4 -k 2 -i ae_in.gl -o out.hdf5 -m 1 -w 0 -q pop_ests.txt -s 20
```

##### c. ESTPOST - pulling out meaningful things from entropy

```{bash eval=FALSE}
/home/skinosian/hts_tools/estpost_h5_entropy -o out_d -p deviance -s 3 -w 1 entropy_ae_k2_2.hdf5
```

-o outfile<br>
-p name of parameter to summarize<br>
-s which summary to perform:<br> 
0 = posterior estimates and credible intervals<br>
1 = histogram of posterior samples<br>
2 = convert to plain text<br>
3 = calculate DIC<br>
4 = MCMC diagnostic<br>
- w write parameter identification to file, boolean<br>


### **Visualizing Admixture**

```{r eval=FALSE}
# read in files from estpost
k2_1 <- read.csv("k2_1.txt", sep = ',', header = T)
k2_1 <- k2_1[,-1]
k2_2 <- read.csv("k2_2.txt", sep = ',', header = F)
k2_3 <- read.csv("k2_3.txt", sep = ',', header = F)
k3_1 <- read.csv("k3_1.txt", sep = ',', header = F)
k3_2 <- read.csv("k3_2.txt", sep = ',', header = F)
k3_3 <- read.csv("k3_3.txt", sep = ',', header = F)
k4_1 <- read.csv("k4_1.txt", sep = ',', header = F)
k5_1 <- read.csv("k5_2.txt", sep = ',', header = F)
names <- read.csv("final_inds.csv", header = F)

# this function averages 3 chains for a given k
avg_k <- function(kval, ninds = 97, chain1, chain2, chain3){
	df <- as.data.frame(matrix(nrow = kval*ninds,  ncol = 4))
	colnames(df)[1:4] <- c("chain1", "chain2", "chain3", "avg")
	df[,1] <- chain1[,1]	
	df[,2] <- chain2[,1]
	df[,3] <- chain3[,1]
	df[,4] <- round(rowMeans(df[sapply(df, is.numeric)]), digits = 6)
	return(df)
}

avg2 <- avg_k(kval = 2, chain1 = k2_1, chain2 = k2_2, chain3 = k2_3)
avg3 <- avg_k(kval = 3, chain1 = k3_1, chain2 = k3_1, chain3= k3_3)
avg4 <- avg_k(kval = 4, chain1 = k4_1, chain2 = k4_1, chain3= k4_1)
avg5 <- avg_k(kval = 5, chain1 = k5_1, chain2 = k5_1, chain3= k5_1)

# add lower bounds of error
#ak5[,5] <- k5[,3]

# add upper bounds of error
#ak5[,6] <- k5[,4]

# column names
#colnames(ak5)[5:6] <- c("LB", "UB")

# makes a data frame with your averaged chains
make_df <- function(kfile, kval, ninds = 97, names){
	x <- 1
	df <- as.data.frame(matrix(nrow = ninds, ncol = kval+x))
	for (i in 1:(kval)){
		df[,i] <- kfile[x:(ninds*i),4]
		x <- x+ninds
	}
	df[,ncol(df)] <- names
	df <- df[order(df[,ncol(df)]),]
	return(df)
}

df2 <- make_df(kfile = avg2, kval = 2, names = names[,3])
df3 <- make_df(kfile = avg3, kval = 3, names = names[,3])
df4 <- make_df(kfile = avg4, kval = 4, names = names[,3])
df5 <- make_df(kfile = avg5, kval = 5, names = names[,3])
k5List <- list(df5[,1:5])

allkList <- list(df2[,1:2], df3[,1:3], df4[,1:4], df5[,1:5])

# original function to plot each chain for a given k
#plot_q_per_chain <- function(kqlist, xlabel, ...){
#	cols <- c('#A8FFFD', '#B862D3','#A39D9D','#FFFF00', '#ff5a5a', '#69C261', '#26CDCD', '#C1C6FF') 	
#	par(mfrow= c(length(kqlist),1), mar=c(4,2,1,1) + 0.1, oma= c(5,0,0,0), mgp= c(0,1,0))
#	chain <- seq(1, length(kqlist), 1) 
#	for(i in 1:length(kqlist)){
#		barplot(t(kqlist[[i]]), beside= F, col= cols, las= 2, axisnames= T, cex.name= 1, cex.axis= 1.2, border= 1.5, space= c(0.05,0.05), yaxt= 'n', ylab= paste("k =", chain[i]+1, sep= ' '), cex.lab= 2, names.arg= xlabel)
#		axis(2, at= c(0, 0.25, 0.5, 0.75, 1), cex.axis= 1, las= 2, pos= -0.2)
#		#arrows(eb2[,1], eb2[,2], angle = 90, code = 3)
#	}
#}

# testing this function
plot_q_per_chain <- function(kqlist, ...){
	cols <- c('#A8FFFD', '#B862D3','#A39D9D','#FFFF00', '#ff5a5a', '#69C261', '#26CDCD', '#C1C6FF') 	
	par(mfrow = c(length(kqlist),1), mar = c(1,3,3,1) + 0.1, oma = c(15,0,0,0), mgp = c(1,1,0))
	chain <- seq(1, length(kqlist), 1) 
	for(i in 1:length(kqlist)){
		barplot(t(kqlist[[i]]), beside= F, col= cols, border = 1, space = 0.05, xaxt = 'n', yaxt = 'n', main = paste("k =", chain[i]+1, sep = ' '), cex.lab = 1.2, cex.main = 1.6)
		# y axis
		axis(2, at = c(0, 0.25, 0.5, 0.75, 1), cex.axis = 1, las = 2, pos = -0.2)
 	}
	# x axis, rotating labels
	text(cex = 1.7, x = (d[,2]-4), y = -0.9, labels = d[,1], xpd=NA, srt=50, font=3)
}

plot_q_per_chain(allkList)


# locations of each column
b <- as.data.frame(matrix(ncol = 1, nrow = 97))
b[,1] <- barplot(t(k2list[[1]]), beside= F, col= cols, cex.name= 1, cex.axis= 1.2, border = 1, space = 0.05, xaxt = 'n', yaxt = 'n', cex.lab = 1, cex.main = 2)

tapply(X=b[,1],INDEX=names[,3],mean)
tapply(X=b[,1],INDEX=names[,3],min)
tapply(X=b[,1],INDEX=names[,3],max)

d <- names & location values

# for k=5 only
plot_q_per_chain <- function(kqlist, ...){
	cols <- c('#A8FFFD', '#B862D3','#A39D9D','#FFFF00', '#ff5a5a', '#69C261', '#26CDCD', '#C1C6FF') 	
	par(mfrow = c(length(kqlist),1), mar = c(1,3,3,1) + 0.1, oma = c(15,0,0,0), mgp = c(1,1,0))
	chain <- seq(1, length(kqlist), 1) 
	for(i in 1:length(kqlist)){
		barplot(t(kqlist[[i]]), beside= F, col= cols, border = 1, space = 0.05, xaxt = 'n', yaxt = 'n', main = "k = 5", cex.lab = 1.2, cex.main = 1.6)
		# y axis
		axis(2, at = c(0, 0.25, 0.5, 0.75, 1), cex.axis = 1, las = 2, pos = -0.2)
 	}
	# x axis, rotating labels
	text(cex = 1.7, x = (d[,2]-4), y = -0.4, labels = d[,1], xpd=NA, srt=50, font=3)
}

plot_q_per_chain(k5List)

for (i in 1:length(d[,1])){
	lines(x = d[i,3:4] , y = rep(-0.05, 2), lwd = 2.5, col = "black", xpd = NA)
}


```

Principal Components Analysis

```{r}
gk2<-matrix(scan("~/Documents/pteridium/gprob/gpk2.txt", n=2128*97,sep=","), nrow=97,ncol=2128,byrow=T)
gk3<-matrix(scan("~/Documents/pteridium/gprob/gpk3.txt", n=2128*97,sep=","), nrow=97,ncol=2128,byrow=T)
gk4<-matrix(scan("~/Documents/pteridium/gprob/gpk4.txt", n=2128*97,sep=","), nrow=97,ncol=2128,byrow=T)
gk5<-matrix(scan("~/Documents/pteridium/gprob/gpk5.txt", n=2128*97,sep=","), nrow=97,ncol=2128,byrow=T)

# average all four Ks
g.avg<-(gk2+gk3+gk4+gk5)/4

# check to make sure everything is correct
#head(g.avg)
dim(g.avg)

# PCA
g.pca<-prcomp(g.avg, scale=TRUE)

cols <- c(rep("black", 2), "red", rep("black", 2), rep("red", 2), rep("black", 3), "red", rep("black", 10), rep("red", 2), "black", "red", rep("black",10), "red", rep("black",4), "green", "blue", "black", "green", "red", "blue", "black", "blue", rep("black", 5), rep("red", 3), "black", "green", "red", rep("black", 6), "red", "red", rep("black", 13), "red", "green", rep("black", 4), "green", "red", rep("black", 5), "red", "green", "red", "black")

# plot
plot(g.pca$x[,1], g.pca$x[,2], col = cols, xlab="PCA1",ylab="PCA2", cex.lab=1.3, cex.axis=1.6, main = "PCA of Pteridium", pch=19)
legend(49, 30, legend=c("P. aquilinum", "P. esculentum", "P. caudatum", "P. semihaustatum"), col=c("black", "red", "green", "blue"), pch=19, cex=0.9)
```

## Program Versions

Below is a list of all program versions used in this analysis. Please note that newer versions of these software packages *may* work for this pipeline, but be aware that usage often changes with new verions. 

[Perl 5](https://www.perl.org/)

[Python 2.7.13](https://www.python.org/downloads/release/python-2713/)

[SAMtools v. 1.5](https://sourceforge.net/projects/samtools/files/samtools/1.5/)

[SEQTK 1.2-r102-dirty](https://github.com/lh3/seqtk)

[VSEARCH 2.4.2](https://github.com/torognes/vsearch)

[BWA 0.7.15](https://sourceforge.net/projects/bio-bwa/files/)

[PicardTools 2.9.0](https://github.com/broadinstitute/picard/releases)

[GATK v.3.8.0](https://software.broadinstitute.org/gatk/download/archive) - [HaplotypeCaller](https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php)

[ENTROPY & ESTPOST](https://github.com/sylviakinosian/Pteridium_GBS_Pipeline/tree/master/entropy)


